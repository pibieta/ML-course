{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi! In this notebook we will do a little \"how *Gradient Boosting* works\" and find out answer for the question:\n",
    "## \"Will performance of GBDT model drop dramatically if we remove the first tree?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataset\n",
    "We will use a very simple dataset: objects will come from 1D normal distribution, we will need to predict class $1$ if the object is positive and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = np.random.randn(5000, 1)\n",
    "y_all = (X_all[:, 0] > 0)*2 - 1  # it creates a +/- 1 feature for each point whether >0 or not\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1, ...,  1, -1,  1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_all[:,0] > 0)*2 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is really simple and can be solved with a single decision stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for a single decision stump: 1.0\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print ('Accuracy for a single decision stump: {}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we will need 800 trees in GBM to classify it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test logloss: 0.00031386465123382074\n"
     ]
    }
   ],
   "source": [
    "# For convenience we will use sklearn's GBM, the situation will be similar with XGBoost and others\n",
    "clf = GradientBoostingClassifier(n_estimators=5000, learning_rate=0.01, max_depth=3, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "print(\"Test logloss: {}\".format(log_loss(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss using all trees:           0.00031386465123382074\n",
      "Logloss using all trees but last:  0.00031386465123382074\n",
      "Logloss using all trees but first: 0.0003202568592776813\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(y_true, scores_pred):\n",
    "    '''\n",
    "        Since we use raw scores we will wrap log_loss \n",
    "        and apply sigmoid to our predictions before computing log_loss itself\n",
    "    '''\n",
    "    return log_loss(y_true, sigmoid(scores_pred))\n",
    "    \n",
    "\n",
    "'''\n",
    "    Get cummulative sum of *decision function* for trees. i-th element is a sum of trees 0...i-1.\n",
    "    We cannot use staged_predict_proba, since we want to maniputate raw scores\n",
    "    (not probabilities). And only in the end convert the scores to probabilities using sigmoid\n",
    "'''\n",
    "cum_preds = np.array([x for x in clf.staged_decision_function(X_test)])[:, :, 0] \n",
    "\n",
    "print (\"Logloss using all trees:           {}\".format(compute_loss(y_test, cum_preds[-1, :])))\n",
    "print (\"Logloss using all trees but last:  {}\".format(compute_loss(y_test, cum_preds[-2, :])))\n",
    "print (\"Logloss using all trees but first: {}\".format(compute_loss(y_test, cum_preds[-1, :] - cum_preds[0, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is a difference, but not as huge as one could expect! Moreover, if we get rid of the first tree — overall model still works! \n",
    "\n",
    "If this is supprising for you — take a look at the plot of cummulative decision function depending on the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEGCAYAAACevtWaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nchcRSSEldxGRFFJyFxFJISV3EZEU+v8a4lq/QS5yTAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1260563c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pick an object of class 1 for visualisation\n",
    "plt.plot(cum_preds[:, y_test == 1][:, 0])\n",
    "\n",
    "plt.xlabel('n_trees')\n",
    "plt.ylabel('Cumulative decision score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See, the decision function improves almost linearly untill about 800 iteration and then stops. And the slope of this line is connected with the learning rate, that we have set in GBM! \n",
    "\n",
    "If you remember the main formula of boosting, you can write something like:\n",
    "    $$ F(x) = const + \\sum\\limits_{i=1}^{n}\\gamma_i h_i(x) $$\n",
    "\n",
    "In our case, $\\gamma_i$ are constant and equal to learning rate $\\eta = 0.01$. And look, it takes about $800$ iterations to get the score $8$, which means at every iteration score goes up for about $0.01$. It means that first 800 terms are approximately equal to $0.01$, and the following are almost $0$. \n",
    "\n",
    "We see, that if we drop the last tree, we lower $F(x)$ by $0$ and if we drop the first tree we lower $F(x)$ by $0.01$, which results in a very very little performance drop.  \n",
    "\n",
    "So, even in the case of simple dataset which can be solved with single decision stump, in GBM we need to sum a lot of trees (roughly $\\frac{1}{\\eta}$) to approximate this golden single decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To prove the point**, let's try a larger learning rate of $8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test logloss: 3.0541825099693313e-06\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=5000, learning_rate=8, max_depth=3, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "print(\"Test logloss: {}\".format(log_loss(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss using all trees:           3.0541825099693313e-06\n",
      "Logloss using all trees but last:  3.06909069883001e-06\n",
      "Logloss using all trees but first: 2.053396855946243\n"
     ]
    }
   ],
   "source": [
    "cum_preds = np.array([x for x in clf.staged_decision_function(X_test)])[:, :, 0] \n",
    "\n",
    "print (\"Logloss using all trees:           {}\".format(compute_loss(y_test, cum_preds[-1, :])))\n",
    "print (\"Logloss using all trees but last:  {}\".format(compute_loss(y_test, cum_preds[-2, :])))\n",
    "print (\"Logloss using all trees but first: {}\".format(compute_loss(y_test, cum_preds[-1, :] - cum_preds[0, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it! Now we see, that it is crucial to have the first tree in the ensemble!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the dataset is synthetic, the similar intuition will work with the real data, except GBM can diverge with high learning rates for a more complex dataset. If you want to play with a little bit more realistic dataset, you can generate it in this notebook with the following code:\n",
    "\n",
    "`X_all, y_all = make_hastie_10_2(random_state=0)` \n",
    "\n",
    "and run the code starting from \"Learn GBM\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all = make_hastie_10_2(random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if a decision tree can classify this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test logloss: 0.18059661273530772\n"
     ]
    }
   ],
   "source": [
    "# For convenience we will use sklearn's GBM, the situation will be similar with XGBoost and others\n",
    "clf = GradientBoostingClassifier(n_estimators=5000, learning_rate=0.01, max_depth=3, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "print(\"Test logloss: {}\".format(log_loss(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss using all trees:           0.14068596291907068\n",
      "Logloss using all trees but last:  0.14072066284698428\n",
      "Logloss using all trees but first: 0.1405820215007183\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(y_true, scores_pred):\n",
    "    '''\n",
    "        Since we use raw scores we will wrap log_loss \n",
    "        and apply sigmoid to our predictions before computing log_loss itself\n",
    "    '''\n",
    "    return log_loss(y_true, sigmoid(scores_pred))\n",
    "    \n",
    "\n",
    "'''\n",
    "    Get cummulative sum of *decision function* for trees. i-th element is a sum of trees 0...i-1.\n",
    "    We cannot use staged_predict_proba, since we want to maniputate raw scores\n",
    "    (not probabilities). And only in the end convert the scores to probabilities using sigmoid\n",
    "'''\n",
    "cum_preds = np.array([x for x in clf.staged_decision_function(X_test)])[:, :, 0] \n",
    "\n",
    "print (\"Logloss using all trees:           {}\".format(compute_loss(y_test, cum_preds[-1, :])))\n",
    "print (\"Logloss using all trees but last:  {}\".format(compute_loss(y_test, cum_preds[-2, :])))\n",
    "print (\"Logloss using all trees but first: {}\".format(compute_loss(y_test, cum_preds[-1, :] - cum_preds[0, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
